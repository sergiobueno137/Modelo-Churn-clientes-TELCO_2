{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a992a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iniciando\n",
    "\n",
    "\n",
    "# Función imputación de outlier\n",
    "# ------\n",
    "\n",
    "def imputar_valores_extremos(df, variable, metodo='media'):\n",
    "    \"\"\"\n",
    "    Imputa valores extremos en una variable de un DataFrame utilizando la media o la mediana.\n",
    "\n",
    "    Parámetros:\n",
    "    df (DataFrame): El DataFrame que contiene la variable a imputar.\n",
    "    variable (str): El nombre de la variable que deseas imputar.\n",
    "    metodo (str): La forma de imputación ('media' o 'mediana'). Por defecto es 'media'.\n",
    "\n",
    "    Retorna:\n",
    "    DataFrame: El DataFrame con la variable imputada.\n",
    "    \"\"\"\n",
    "    if metodo not in ['media', 'mediana']:\n",
    "        raise ValueError(\"El método debe ser 'media' o 'mediana'\")\n",
    "\n",
    "    # Calcular la media o la mediana\n",
    "    if metodo == 'media':\n",
    "        valor_imputacion = df[variable].mean()\n",
    "    else:\n",
    "        valor_imputacion = df[variable].median()\n",
    "\n",
    "    # Identificar valores extremos (usando una regla de 3 veces la desviación estándar)\n",
    "    limite_inferior = df[variable].mean() - 3 * df[variable].std()\n",
    "    limite_superior = df[variable].mean() + 3 * df[variable].std()\n",
    "\n",
    "    # Imputar valores extremos\n",
    "    df[variable] = np.where(\n",
    "        (df[variable] < limite_inferior) | (df[variable] > limite_superior),\n",
    "        valor_imputacion,\n",
    "        df[variable]\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Función imputación perdidos\n",
    "# ------\n",
    "\n",
    "def imputar_valores(df, variable, metodo='media', valor_especifico=None):\n",
    "    \"\"\"\n",
    "    Imputa valores perdidos en una columna de un DataFrame según el método especificado.\n",
    "\n",
    "    Parámetros:\n",
    "    df (pd.DataFrame): El DataFrame en el que se imputarán los valores.\n",
    "    variable (str): El nombre de la columna a imputar.\n",
    "    metodo (str): El método de imputación ('media', 'mediana', 'moda', 'valor_especifico').\n",
    "    valor_especifico: El valor específico a usar para la imputación (relevante solo si 'metodo' es 'valor_especifico').\n",
    "\n",
    "    Retorna:\n",
    "    pd.DataFrame: El DataFrame con la variable imputada.\n",
    "    \"\"\"\n",
    "\n",
    "    if metodo == 'media':\n",
    "        imputacion = df[variable].mean()\n",
    "    elif metodo == 'mediana':\n",
    "        imputacion = df[variable].median()\n",
    "    elif metodo == 'moda':\n",
    "        imputacion = df[variable].mode()[0]\n",
    "    elif metodo == 'valor_especifico':\n",
    "        if valor_especifico is None:\n",
    "            raise ValueError(\"Debe proporcionar un valor específico para la imputación.\")\n",
    "        imputacion = valor_especifico\n",
    "    else:\n",
    "        raise ValueError(\"Método de imputación no reconocido. Use 'media', 'mediana', 'moda' o 'valor_especifico'.\")\n",
    "\n",
    "    df[variable].fillna(imputacion, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Funcion graficadora confusion_marix\n",
    "# ---\n",
    "def confusion_matrix_graph(cm):\n",
    "  plt.figure(figsize=(8, 6))\n",
    "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "              xticklabels=['No', 'Yes'],\n",
    "              yticklabels=['No', 'Yes'])\n",
    "  plt.title('Matriz de Confusión')\n",
    "  plt.xlabel('Predicción')\n",
    "  plt.ylabel('Realidad')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "# Funcion ROC curve\n",
    "# ---\n",
    "def roc_curve_graph(y,prob):\n",
    "  # Obtener las probabilidades de la clase positiva\n",
    "  y_prob = prob[:, 1]  # Probabilidades de la clase 1\n",
    "\n",
    "  # Calcular la curva ROC\n",
    "  fpr, tpr, thresholds = roc_curve(y,  y_prob)\n",
    "\n",
    "  # Calcular el área bajo la curva (AUC)\n",
    "  roc_auc = auc(fpr, tpr)\n",
    "\n",
    "  # Graficar la curva ROC\n",
    "  plt.figure(figsize=(8, 6))\n",
    "  plt.plot(fpr, tpr, color='blue', label=f'AUC = {roc_auc:.2f}')\n",
    "  plt.plot([0, 1], [0, 1], color='red', linestyle='--')  # Línea de referencia\n",
    "  plt.title('Curva ROC')\n",
    "  plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
    "  plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
    "  plt.legend(loc='lower right')\n",
    "  plt.grid()\n",
    "  plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f831a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scripts del Proyecto\n",
    "\n",
    "# Script 1: Preparacion de datos para el entrenamiento\n",
    "#---------------------------------------------------------\n",
    "\n",
    "# carga de datos\n",
    "#Importamos las librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Leemos la tabla de entrenamiento\n",
    "data = pd.read_csv(\"..\\data\\Data_Customer_Churn.csv\",sep = ';')\n",
    "\n",
    "# 1 Limpieza de datos\n",
    "\n",
    "## Separando las variables segun su tipo para un correcta lectura\n",
    "# Lista de variables numéricas\n",
    "numeric_vars = data.select_dtypes(include=['number']).columns.tolist()\n",
    "# Lista de variables categóricas\n",
    "categorical_vars = data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Actualizamos variables numericas\n",
    "data[\"SeniorCitizen\"] = data[\"SeniorCitizen\"].astype(\"str\")\n",
    "numeric_vars.remove('SeniorCitizen')\n",
    "categorical_vars.append('SeniorCitizen')\n",
    "numeric_vars, categorical_vars\n",
    "# Analizando variables categóricas\n",
    "# Iterar sobre las columnas del DataFrame\n",
    "for column in data.columns:\n",
    "  if data[column].dtype == 'object' or data[column].dtype.name == 'category':\n",
    "    print(f\"Resumen de porcentajes para la variable '{column}':\\n\")\n",
    "    print(data[column].value_counts(normalize=True) * 100)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Actualizamos variables categoricas\n",
    "data[\"TotalCharges\"] = data[\"TotalCharges\"].replace(\" \",np.nan)\n",
    "data[\"TotalCharges\"] = data[\"TotalCharges\"].astype(\"float\")\n",
    "categorical_vars.remove('TotalCharges')\n",
    "numeric_vars.append('TotalCharges')\n",
    "\n",
    "# Presencia de valores perdidos\n",
    "for column in data.columns:\n",
    "    missing_percentage = data[column].isnull().mean() * 100\n",
    "    print(f'{column}: {missing_percentage:.2f}%')\n",
    "\n",
    "# Tenemos valores en nuestra tabla que son espacios en blanco no necesariamente son valores nulos (TIP OPCIONAL)\n",
    "data = imputar_valores(data,'TotalCharges',metodo='mediana')\n",
    "\n",
    "for column in data.columns:\n",
    "    missing_percentage = data[column].isnull().mean() * 100\n",
    "    print(f'{column}: {missing_percentage:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2 Preprocesamiento de datos\n",
    "\n",
    "# Retirando la variable target de la lista de vaiables categoricas\n",
    "categorical_vars.remove('Churn')\n",
    "categorical_vars.remove('customerID')\n",
    "# Guardar todas las variables categoricas en un solo lugar\n",
    "cat_cols = data[categorical_vars]\n",
    "num_cols = data[numeric_vars]\n",
    "# Generar variables para las dos columnas que omiti de mi mapeo de variables cualitativas y cuantitativas\n",
    "id_customer = data[\"customerID\"]\n",
    "label = data[\"Churn\"]\n",
    "\n",
    "# Transformacion de variables categoricas a numericas\n",
    "# Label encoding (Target Encoding) : Cambiar en la misma columna el valor categorico a numerico\n",
    "# La variable target categorica solo puede ser transformada con el target encoding\n",
    "label = label.apply(lambda x: 1 if x == \"Yes\" else 0) # Yes - 1, No -0\n",
    "\n",
    "\n",
    "# Analizando importancia de variables en un modelo simple\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Separar las variables de entrada y la target\n",
    "X = data[['tenure','TotalCharges']]\n",
    "y = data['Churn']\n",
    "# Entrenar un árbol de decisión\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X, y)\n",
    "# Obtener la importancia de las variables\n",
    "importances = tree.feature_importances_\n",
    "print(f'Importancia de tenure: {importances[0]}')\n",
    "print(f'Importancia de TotalCharges: {importances[1]}')\n",
    "\n",
    "# retirando la variable que menos aporta\n",
    "numeric_vars.remove('tenure')\n",
    "numeric_vars\n",
    "del num_cols['tenure']\n",
    "\n",
    "#transformamos las variables categóricas a numéricas\n",
    "cat_cols = pd.get_dummies(data = cat_cols)\n",
    "\n",
    "# Creamos una nueva data donde concatenamos la data original con la data que contiene las nuevas variables de one-hot-encoding:\n",
    "# que será la data final para aplicar los modelos\n",
    "df_complete = pd.concat([num_cols, cat_cols, label], axis=1)\n",
    "df_complete.to_csv(\"../data/processed/churn_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee556a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Script 2: Código de Entrenamiento\n",
    "#---------------------------------------\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score,roc_curve # ROC a diferencia del accuracy te da un valor justo de precision para datos desbalanceados\n",
    "# Particionado de datos\n",
    "# Al dataset o set de entrenamiento le retiramos la variable dependiente o target\n",
    "X = df_complete.drop(\"Churn\",axis=1) # covariables\n",
    "Y = df_complete['Churn'] # target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_res, X_test, y_train_res, y_test = train_test_split(X, Y, test_size = 0.3, random_state=10)\n",
    "\n",
    "\n",
    "pip install imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# Aplicar SMOTE al conjunto de entrenamiento\n",
    "# Instanciar SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.6, random_state=42)   # Instanciamos el SMOTE\n",
    "# Aplicar SMOTE\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_res, y_train_res)\n",
    "# Seleccionamos el tipo de remuestreo que mejor nos beneficie\n",
    "X_train = X_train_smote\n",
    "y_train = y_train_smote\n",
    "\n",
    "# Entrenamos el modelo con toda la muestra\n",
    "LG = LogisticRegression()\n",
    "LG.fit(X_train, y_train)\n",
    "\n",
    "y_train_predict_lgr =  LG.predict(X_train)\n",
    "y_test_predict_lgr =  LG.predict(X_test)\n",
    "\n",
    "y_train_predict_proba_lgr =  LG.predict_proba(X_train)\n",
    "y_test_predict_proba_lgr =  LG.predict_proba(X_test)\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "cm_lgr = confusion_matrix(y_train, y_train_predict_lgr)\n",
    "cm_lgr\n",
    "confusion_matrix_graph(cm_lgr)\n",
    "# las métricas\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_train,y_train_predict_lgr))\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test,y_test_predict_lgr))\n",
    "roc_curve_graph(y_train,y_train_predict_proba_lgr)\n",
    "\n",
    "# Guardamos el modelo entrenado para usarlo en produccion\n",
    "import pickle\n",
    "filename = '..\\models\\modelo_LG.sav'\n",
    "pickle.dump(LG, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680a12dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Script 3: Preparación de Datos de Validación\n",
    "#------------------------------------------------\n",
    "# carga de datos\n",
    "#Importamos las librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Leemos la tabla de entrenamiento\n",
    "data = pd.read_csv(\"..\\data\\Data_Customer_Churn_new.csv\",sep = ';')\n",
    "\n",
    "# 1 Limpieza de datos\n",
    "\n",
    "## Separando las variables segun su tipo para un correcta lectura\n",
    "# Lista de variables numéricas\n",
    "numeric_vars = data.select_dtypes(include=['number']).columns.tolist()\n",
    "# Lista de variables categóricas\n",
    "categorical_vars = data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Actualizamos variables numericas\n",
    "data[\"SeniorCitizen\"] = data[\"SeniorCitizen\"].astype(\"str\")\n",
    "numeric_vars.remove('SeniorCitizen')\n",
    "categorical_vars.append('SeniorCitizen')\n",
    "numeric_vars, categorical_vars\n",
    "# Analizando variables categóricas\n",
    "# Iterar sobre las columnas del DataFrame\n",
    "for column in data.columns:\n",
    "  if data[column].dtype == 'object' or data[column].dtype.name == 'category':\n",
    "    print(f\"Resumen de porcentajes para la variable '{column}':\\n\")\n",
    "    print(data[column].value_counts(normalize=True) * 100)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Actualizamos variables categoricas\n",
    "data[\"TotalCharges\"] = data[\"TotalCharges\"].replace(\" \",np.nan)\n",
    "data[\"TotalCharges\"] = data[\"TotalCharges\"].astype(\"float\")\n",
    "categorical_vars.remove('TotalCharges')\n",
    "numeric_vars.append('TotalCharges')\n",
    "\n",
    "# Presencia de valores perdidos\n",
    "for column in data.columns:\n",
    "    missing_percentage = data[column].isnull().mean() * 100\n",
    "    print(f'{column}: {missing_percentage:.2f}%')\n",
    "\n",
    "# Tenemos valores en nuestra tabla que son espacios en blanco no necesariamente son valores nulos (TIP OPCIONAL)\n",
    "data = imputar_valores(data,'TotalCharges',metodo='mediana')\n",
    "\n",
    "for column in data.columns:\n",
    "    missing_percentage = data[column].isnull().mean() * 100\n",
    "    print(f'{column}: {missing_percentage:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2 Preprocesamiento de datos\n",
    "\n",
    "# Retirando la variable target de la lista de vaiables categoricas\n",
    "categorical_vars.remove('Churn')\n",
    "categorical_vars.remove('customerID')\n",
    "# Guardar todas las variables categoricas en un solo lugar\n",
    "cat_cols = data[categorical_vars]\n",
    "num_cols = data[numeric_vars]\n",
    "# Generar variables para las dos columnas que omiti de mi mapeo de variables cualitativas y cuantitativas\n",
    "id_customer = data[\"customerID\"]\n",
    "label = data[\"Churn\"]\n",
    "\n",
    "# Transformacion de variables categoricas a numericas\n",
    "# Label encoding (Target Encoding) : Cambiar en la misma columna el valor categorico a numerico\n",
    "# La variable target categorica solo puede ser transformada con el target encoding\n",
    "label = label.apply(lambda x: 1 if x == \"Yes\" else 0) # Yes - 1, No -0\n",
    "\n",
    "# Analizando importancia de variables en un modelo simple\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Separar las variables de entrada y la target\n",
    "X = data[['tenure','TotalCharges']]\n",
    "y = data['Churn']\n",
    "# Entrenar un árbol de decisión\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X, y)\n",
    "# Obtener la importancia de las variables\n",
    "importances = tree.feature_importances_\n",
    "print(f'Importancia de tenure: {importances[0]}')\n",
    "print(f'Importancia de TotalCharges: {importances[1]}')\n",
    "\n",
    "# retirando la variable que menos aporta\n",
    "numeric_vars.remove('tenure')\n",
    "numeric_vars\n",
    "del num_cols['tenure']\n",
    "\n",
    "#transformamos las variables categóricas a numéricas\n",
    "cat_cols = pd.get_dummies(data = cat_cols)\n",
    "\n",
    "# Creamos una nueva data donde concatenamos la data original con la data que contiene las nuevas variables de one-hot-encoding:\n",
    "# que será la data final para aplicar los modelos\n",
    "df_complete = pd.concat([num_cols, cat_cols, label], axis=1)\n",
    "df_complete.to_csv(\"../data/processed/churn_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881887dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Script 4: Código de Validación\n",
    "#--------------------------------\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cargar la tabla transformada\n",
    "df = pd.read_csv(\"../data/processed/churn_val.csv\")\n",
    "X_test = df.drop(['Churn'],axis=1)\n",
    "y_test = df[['Churn']]\n",
    "# Leemos el modelo entrenado!\n",
    "filename = '..\\models\\modelo_LG.sav'\n",
    "pickle.dump(LG, open(filename, 'wb'))\n",
    "\n",
    "# Predecimos sobre el set de datos de implementacion con el modelo entrenado\n",
    "y_pred_test=model.predict(df.drop(['Churn'],axis=1)) \n",
    "## Metricas de validación\n",
    "def calc_metrics(y_test,y_pred_test):\n",
    "    cm_test = confusion_matrix(y_test,y_pred_test)\n",
    "    print(\"Matriz de confusion: \")\n",
    "    print(cm_test)\n",
    "    accuracy_test=accuracy_score(y_test,y_pred_test)\n",
    "    print(\"Accuracy: \", accuracy_test)\n",
    "    precision_test=precision_score(y_test,y_pred_test)\n",
    "    print(\"Precision: \", precision_test)\n",
    "    recall_test=recall_score(y_test,y_pred_test)\n",
    "    print(\"Recall: \", recall_test)\n",
    "def save_plot(title):\n",
    "    plt.title(title)\n",
    "    fig = plt.gcf()\n",
    "    filename = title.replace(\" \", \"_\").lower()\n",
    "    fig.savefig('{}'.format(filename), dpi=500)\n",
    "    plt.clf()\n",
    "plot_confusion_matrix(model, X_test, y_test)\n",
    "save_plot('Confusion Matrix')\n",
    "plot_roc_curve(model, X_test, y_test)\n",
    "save_plot('ROC Curve')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
